{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ0gV4CnQ5sH"
      },
      "source": [
        "# =====================================================================\n",
        "# üìå CELL 1 ‚Äî Install dependencies (RUN THIS FIRST IN COLAB)\n",
        "# =====================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-6Ximb9Q5sI",
        "outputId": "aa8faafe-e881-4bf7-abd3-b12f759872f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.10.5)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=256040057 sha256=f25da18657a87fc83dc1bfb8b7751b82246e9db355510226b674fd437c34b5fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2 flash-attn-2.8.3\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install timm bitsandbytes flash-attn --no-build-isolation\n",
        "!pip install matplotlib pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEdNWRoiQ5sI"
      },
      "source": [
        "# =====================================================================\n",
        "# üìå CELL 2 ‚Äî Imports, device setup, Tiny-ImageNet paths\n",
        "# =====================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLDBVWNLQ5sI",
        "outputId": "dc93bf76-7d7d-4381-fb6e-37b4c5d3f9fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ FlashAttention-2 available.\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "from statistics import mean\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import timm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check flash-attn\n",
        "try:\n",
        "    from flash_attn import flash_attn_func\n",
        "    FLASH_ATTENTION_AVAILABLE = True\n",
        "    print(\"‚úÖ FlashAttention-2 available.\")\n",
        "except Exception as e:\n",
        "    FLASH_ATTENTION_AVAILABLE = False\n",
        "    print(\"‚ùå FlashAttention-2 NOT available:\", e)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DATA_ROOT = \"/content/tiny-imagenet-200\"\n",
        "RESULTS_DIR = Path(\"/content/results_vit_quant_fa2\")\n",
        "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_SIZE = 224\n",
        "MAX_VAL_BATCHES = 10   # small subset for Colab speed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdsO_52mQ5sI"
      },
      "source": [
        "# =====================================================================\n",
        "# üìå CELL 3 ‚Äî Download + reorganize Tiny-ImageNet\n",
        "# =====================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDGZbjw1Q5sJ",
        "outputId": "f0d9d9d4-ee82-409e-dbf6-3908622b63a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiny-ImageNet already present.\n",
            "val/ already organized.\n",
            "Val samples: 10000 | Classes: 200\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(DATA_ROOT):\n",
        "    print(\"üì• Downloading Tiny-ImageNet...\")\n",
        "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "    zip_path = \"/content/tiny-imagenet-200.zip\"\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    print(\"Extracting...\")\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(\"/content\")\n",
        "    print(\"Done.\")\n",
        "else:\n",
        "    print(\"Tiny-ImageNet already present.\")\n",
        "\n",
        "# Fix val/ to ImageFolder format\n",
        "val_dir = Path(DATA_ROOT) / \"val\"\n",
        "images_dir = val_dir / \"images\"\n",
        "ann_path = val_dir / \"val_annotations.txt\"\n",
        "\n",
        "if images_dir.exists():\n",
        "    print(\"üîß Organizing val/...\")\n",
        "    import shutil\n",
        "    with open(ann_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            img, wnid = line.strip().split(\"\\t\")[:2]\n",
        "            cls_dir = val_dir / wnid\n",
        "            cls_dir.mkdir(exist_ok=True)\n",
        "            src = images_dir / img\n",
        "            dst = cls_dir / img\n",
        "            if src.exists():\n",
        "                shutil.move(str(src), str(dst))\n",
        "    print(\"val/ reorganized.\")\n",
        "else:\n",
        "    print(\"val/ already organized.\")\n",
        "\n",
        "# ... your download + reorg code above ...\n",
        "\n",
        "# If 'images' directory is empty, remove it so ImageFolder doesn't treat it as a class\n",
        "if images_dir.exists():\n",
        "    import shutil\n",
        "    if not any(images_dir.glob(\"*\")):\n",
        "        print(\"Removing empty 'val/images' directory to avoid ImageFolder error.\")\n",
        "        shutil.rmtree(images_dir)\n",
        "\n",
        "# Validation loader\n",
        "val_transforms = T.Compose([\n",
        "    T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225),\n",
        "    ),\n",
        "])\n",
        "\n",
        "val_dataset = torchvision.datasets.ImageFolder(\n",
        "    root=os.path.join(DATA_ROOT, \"val\"),\n",
        "    transform=val_transforms,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "num_classes = len(val_dataset.classes)\n",
        "print(\"Val samples:\", len(val_dataset), \"| Classes:\", num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVimFjGdQ5sJ"
      },
      "source": [
        "# =====================================================================\n",
        "# üìå CELL 4 ‚Äî Accuracy + latency utilities\n",
        "# =====================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d3pa74_XQ5sJ"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_accuracy(model, loader, max_batches=None):\n",
        "    model.eval()\n",
        "    top1 = top5 = total = 0\n",
        "    use_half = next(model.parameters()).dtype == torch.float16\n",
        "\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        if use_half:\n",
        "            x = x.half()\n",
        "\n",
        "        out = model(x)\n",
        "        _, p1 = out.topk(1, dim=1)\n",
        "        _, p5 = out.topk(5, dim=1)\n",
        "\n",
        "        total += y.size(0)\n",
        "        top1 += (p1.squeeze() == y).sum().item()\n",
        "        top5 += (p5 == y.unsqueeze(1)).any(dim=1).sum().item()\n",
        "\n",
        "        if max_batches and (idx + 1) >= max_batches:\n",
        "            break\n",
        "\n",
        "    return top1 / total * 100.0, top5 / total * 100.0\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def measure_latency(model, input_shape=(1,3,224,224), warmup=10, iters=50):\n",
        "    model.eval()\n",
        "    dummy = torch.randn(input_shape, device=device)\n",
        "    if next(model.parameters()).dtype == torch.float16:\n",
        "        dummy = dummy.half()\n",
        "\n",
        "    for _ in range(warmup):\n",
        "        model(dummy)\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    times = []\n",
        "    for _ in range(iters):\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        t0 = time.time()\n",
        "        model(dummy)\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        times.append((time.time() - t0) * 1000)\n",
        "\n",
        "    times_s = sorted(times)\n",
        "    return {\n",
        "        \"mean_ms\": mean(times),\n",
        "        \"p50_ms\": times_s[int(0.5*len(times))],\n",
        "        \"p95_ms\": times_s[int(0.95*len(times))-1],\n",
        "        \"p99_ms\": times_s[int(0.99*len(times))-1],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhHi5knnQ5sJ"
      },
      "source": [
        "# =====================================================================\n",
        "# üìå CELL 5 ‚Äî FlashAttention-2 wrapper for timm ViT\n",
        "# =====================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3rccjjvLQ5sJ"
      },
      "outputs": [],
      "source": [
        "class FlashAttentionVit(nn.Module):\n",
        "    def __init__(self, base):\n",
        "        super().__init__()\n",
        "        self.qkv = base.qkv\n",
        "        self.proj = base.proj\n",
        "        self.num_heads = base.num_heads\n",
        "        self.scale = base.scale\n",
        "        self.attn_drop = getattr(base, \"attn_drop\", nn.Identity())\n",
        "        self.proj_drop = getattr(base, \"proj_drop\", nn.Identity())\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(B, N, 3, self.num_heads, C//self.num_heads)\n",
        "        qkv = qkv.permute(2,0,3,1,4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        q = q.permute(0,2,1,3)\n",
        "        k = k.permute(0,2,1,3)\n",
        "        v = v.permute(0,2,1,3)\n",
        "\n",
        "        q = q.half()\n",
        "        k = k.half()\n",
        "        v = v.half()\n",
        "\n",
        "        out = flash_attn_func(q, k, v, dropout_p=0.0, softmax_scale=None, causal=False)\n",
        "        out = out.permute(0,2,1,3).reshape(B,N,C)\n",
        "        out = self.proj(out)\n",
        "        return self.proj_drop(out)\n",
        "\n",
        "\n",
        "def apply_flash_attention_to_vit(model):\n",
        "    # Guard: require CUDA + Ampere (SM80) or newer, and flash_attn installed\n",
        "    if device.type != \"cuda\":\n",
        "        print(\"‚ö†Ô∏è No CUDA device ‚Äî skipping FlashAttention.\")\n",
        "        return model\n",
        "\n",
        "    if not FLASH_ATTENTION_AVAILABLE:\n",
        "        print(\"‚ö†Ô∏è FA2 not available ‚Äî skipping.\")\n",
        "        return model\n",
        "\n",
        "    try:\n",
        "        major, _ = torch.cuda.get_device_capability()\n",
        "    except Exception:\n",
        "        major = 0\n",
        "\n",
        "    if major < 8:\n",
        "        print(\"‚ö†Ô∏è FlashAttention requires Ampere (SM80) or newer GPU ‚Äî skipping.\")\n",
        "        return model\n",
        "\n",
        "    model = model.to(device).half()\n",
        "    replaced = 0\n",
        "\n",
        "    def recurse(m):\n",
        "        nonlocal replaced\n",
        "        for name, child in list(m.named_children()):\n",
        "            if child.__class__.__name__ == \"Attention\":\n",
        "                setattr(m, name, FlashAttentionVit(child))\n",
        "                replaced += 1\n",
        "            else:\n",
        "                recurse(child)\n",
        "\n",
        "    recurse(model)\n",
        "    print(f\"‚ú® FlashAttention integrated into {replaced} blocks.\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVdPgJDZQ5sJ"
      },
      "source": [
        "# =====================================================================\n",
        "# üìå CELL 6 ‚Äî bitsandbytes quantization + model builders\n",
        "# =====================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8T2eAPkoQ5sJ"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes as bnb\n",
        "\n",
        "def quantize_linear_8bit(m: nn.Linear):\n",
        "    q = bnb.nn.Linear8bitLt(m.in_features, m.out_features, bias=m.bias is not None)\n",
        "    q.weight.data = m.weight.data.clone()\n",
        "    if m.bias is not None:\n",
        "        q.bias.data = m.bias.data.clone()\n",
        "    return q\n",
        "\n",
        "def quantize_linear_4bit(m: nn.Linear):\n",
        "    q = bnb.nn.Linear4bit(\n",
        "        m.in_features, m.out_features,\n",
        "        bias=m.bias is not None,\n",
        "        quant_type=\"nf4\",\n",
        "        compute_dtype=torch.float16,\n",
        "    )\n",
        "    q.weight.data = m.weight.data.clone()\n",
        "    if m.bias is not None:\n",
        "        q.bias.data = m.bias.data.clone()\n",
        "    return q\n",
        "\n",
        "def _quantize_block(block, bits):\n",
        "    for name, child in list(block.named_children()):\n",
        "        if isinstance(child, nn.Linear):\n",
        "            new = quantize_linear_4bit(child) if bits==4 else quantize_linear_8bit(child)\n",
        "            setattr(block, name, new)\n",
        "        else:\n",
        "            _quantize_block(child, bits)\n",
        "\n",
        "def apply_quantization_to_vit(model, bits):\n",
        "    for blk in model.blocks:\n",
        "        _quantize_block(blk, bits)\n",
        "    print(f\"üßä Quantized ViT blocks to {bits}-bit.\")\n",
        "    return model\n",
        "\n",
        "def build_vit_baseline(use_half=False):\n",
        "    m = timm.create_model(\"vit_large_patch16_224\", pretrained=True, num_classes=num_classes)\n",
        "    m.to(device)\n",
        "    return m.half() if use_half else m\n",
        "\n",
        "def build_vit_quantized(bits, use_fa2):\n",
        "    m = build_vit_baseline(use_half=use_fa2)\n",
        "    m = apply_quantization_to_vit(m, bits)\n",
        "    if use_fa2:\n",
        "        m = apply_flash_attention_to_vit(m)\n",
        "    return m.to(device)\n",
        "\n",
        "def get_model_registry():\n",
        "    return {\n",
        "        \"vit_fp32_baseline\": {\n",
        "            \"desc\": \"FP32 baseline\",\n",
        "            \"builder\": lambda: build_vit_baseline(use_half=False),\n",
        "            \"bits\": None,\n",
        "            \"fa2\": False,\n",
        "        },\n",
        "        \"vit_4bit_fa2\": {\n",
        "            \"desc\": \"4-bit + FlashAttention-2\",\n",
        "            \"builder\": lambda: build_vit_quantized(4, True),\n",
        "            \"bits\": 4,\n",
        "            \"fa2\": True,\n",
        "        },\n",
        "        \"vit_8bit_fa2\": {\n",
        "            \"desc\": \"8-bit + FlashAttention-2\",\n",
        "            \"builder\": lambda: build_vit_quantized(8, True),\n",
        "            \"bits\": 8,\n",
        "            \"fa2\": True,\n",
        "        },\n",
        "        \"vit_4bit_sdpa\": {\n",
        "            \"desc\": \"4-bit SDPA\",\n",
        "            \"builder\": lambda: build_vit_quantized(4, False),\n",
        "            \"bits\": 4,\n",
        "            \"fa2\": False,\n",
        "        },\n",
        "        \"vit_8bit_sdpa\": {\n",
        "            \"desc\": \"8-bit SDPA\",\n",
        "            \"builder\": lambda: build_vit_quantized(8, False),\n",
        "            \"bits\": 8,\n",
        "            \"fa2\": False,\n",
        "        },\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L4_XQ7kQ5sJ"
      },
      "source": [
        "# =====================================================================\n",
        "# üìå CELL 7 ‚Äî Stage 1 Benchmark (ALL 5 MODELS)\n",
        "# =====================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "fePpj79ZQ5sJ",
        "outputId": "1c143851-7ce2-4af9-bff2-42b5c4930cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "MODEL: vit_fp32_baseline | FP32 baseline\n",
            "================================================================================\n",
            "Params: 303506632\n",
            "Accuracy ‚Äî Top-1: 0.31% | Top-5: 1.41%\n",
            "Latency (ms): {'mean_ms': 58.863863945007324, 'p50_ms': 56.79464340209961, 'p95_ms': 66.55097007751465, 'p99_ms': 71.10881805419922}\n",
            "================================================================================\n",
            "MODEL: vit_4bit_fa2 | 4-bit + FlashAttention-2\n",
            "================================================================================\n",
            "üßä Quantized ViT blocks to 4-bit.\n",
            "‚ö†Ô∏è FlashAttention requires Ampere (SM80) or newer GPU ‚Äî skipping.\n",
            "Params: 152511688\n",
            "Accuracy ‚Äî Top-1: 0.78% | Top-5: 3.59%\n",
            "Latency (ms): {'mean_ms': 35.03471851348877, 'p50_ms': 34.30628776550293, 'p95_ms': 38.0706787109375, 'p99_ms': 44.88730430603027}\n",
            "================================================================================\n",
            "MODEL: vit_8bit_fa2 | 8-bit + FlashAttention-2\n",
            "================================================================================\n",
            "üßä Quantized ViT blocks to 8-bit.\n",
            "‚ö†Ô∏è FlashAttention requires Ampere (SM80) or newer GPU ‚Äî skipping.\n",
            "Params: 303506632\n",
            "Accuracy ‚Äî Top-1: 2.50% | Top-5: 6.56%\n",
            "Latency (ms): {'mean_ms': 92.11071014404297, 'p50_ms': 68.52865219116211, 'p95_ms': 220.32570838928223, 'p99_ms': 239.84813690185547}\n",
            "================================================================================\n",
            "MODEL: vit_4bit_sdpa | 4-bit SDPA\n",
            "================================================================================\n",
            "üßä Quantized ViT blocks to 4-bit.\n",
            "Params: 152511688\n",
            "Accuracy ‚Äî Top-1: 3.59% | Top-5: 9.22%\n",
            "Latency (ms): {'mean_ms': 42.05423355102539, 'p50_ms': 40.71807861328125, 'p95_ms': 48.50888252258301, 'p99_ms': 53.93815040588379}\n",
            "================================================================================\n",
            "MODEL: vit_8bit_sdpa | 8-bit SDPA\n",
            "================================================================================\n",
            "üßä Quantized ViT blocks to 8-bit.\n",
            "Params: 303506632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy ‚Äî Top-1: 0.00% | Top-5: 1.25%\n",
            "Latency (ms): {'mean_ms': 60.82291126251221, 'p50_ms': 60.050010681152344, 'p95_ms': 66.54858589172363, 'p99_ms': 70.20211219787598}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               model                      desc  bits    fa2     top1     top5  \\\n",
              "0  vit_fp32_baseline             FP32 baseline   NaN  False  0.31250  1.40625   \n",
              "1       vit_4bit_fa2  4-bit + FlashAttention-2   4.0   True  0.78125  3.59375   \n",
              "2       vit_8bit_fa2  8-bit + FlashAttention-2   8.0   True  2.50000  6.56250   \n",
              "3      vit_4bit_sdpa                4-bit SDPA   4.0  False  3.59375  9.21875   \n",
              "4      vit_8bit_sdpa                8-bit SDPA   8.0  False  0.00000  1.25000   \n",
              "\n",
              "   lat_mean_ms    lat_p50     lat_p95     lat_p99  \n",
              "0    58.863864  56.794643   66.550970   71.108818  \n",
              "1    35.034719  34.306288   38.070679   44.887304  \n",
              "2    92.110710  68.528652  220.325708  239.848137  \n",
              "3    42.054234  40.718079   48.508883   53.938150  \n",
              "4    60.822911  60.050011   66.548586   70.202112  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b11f8aa0-eb9a-48d0-876b-c53bc498ecc7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>desc</th>\n",
              "      <th>bits</th>\n",
              "      <th>fa2</th>\n",
              "      <th>top1</th>\n",
              "      <th>top5</th>\n",
              "      <th>lat_mean_ms</th>\n",
              "      <th>lat_p50</th>\n",
              "      <th>lat_p95</th>\n",
              "      <th>lat_p99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vit_fp32_baseline</td>\n",
              "      <td>FP32 baseline</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>0.31250</td>\n",
              "      <td>1.40625</td>\n",
              "      <td>58.863864</td>\n",
              "      <td>56.794643</td>\n",
              "      <td>66.550970</td>\n",
              "      <td>71.108818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>vit_4bit_fa2</td>\n",
              "      <td>4-bit + FlashAttention-2</td>\n",
              "      <td>4.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.78125</td>\n",
              "      <td>3.59375</td>\n",
              "      <td>35.034719</td>\n",
              "      <td>34.306288</td>\n",
              "      <td>38.070679</td>\n",
              "      <td>44.887304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>vit_8bit_fa2</td>\n",
              "      <td>8-bit + FlashAttention-2</td>\n",
              "      <td>8.0</td>\n",
              "      <td>True</td>\n",
              "      <td>2.50000</td>\n",
              "      <td>6.56250</td>\n",
              "      <td>92.110710</td>\n",
              "      <td>68.528652</td>\n",
              "      <td>220.325708</td>\n",
              "      <td>239.848137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vit_4bit_sdpa</td>\n",
              "      <td>4-bit SDPA</td>\n",
              "      <td>4.0</td>\n",
              "      <td>False</td>\n",
              "      <td>3.59375</td>\n",
              "      <td>9.21875</td>\n",
              "      <td>42.054234</td>\n",
              "      <td>40.718079</td>\n",
              "      <td>48.508883</td>\n",
              "      <td>53.938150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vit_8bit_sdpa</td>\n",
              "      <td>8-bit SDPA</td>\n",
              "      <td>8.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.25000</td>\n",
              "      <td>60.822911</td>\n",
              "      <td>60.050011</td>\n",
              "      <td>66.548586</td>\n",
              "      <td>70.202112</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b11f8aa0-eb9a-48d0-876b-c53bc498ecc7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b11f8aa0-eb9a-48d0-876b-c53bc498ecc7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b11f8aa0-eb9a-48d0-876b-c53bc498ecc7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b1aa0d2c-1752-4fc8-993b-4dd77d169d56\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b1aa0d2c-1752-4fc8-993b-4dd77d169d56')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b1aa0d2c-1752-4fc8-993b-4dd77d169d56 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_515fb223-ba9d-46cb-a4aa-14227f028b9c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_515fb223-ba9d-46cb-a4aa-14227f028b9c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"vit_4bit_fa2\",\n          \"vit_8bit_sdpa\",\n          \"vit_8bit_fa2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"desc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"4-bit + FlashAttention-2\",\n          \"8-bit SDPA\",\n          \"8-bit + FlashAttention-2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bits\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.309401076758503,\n        \"min\": 4.0,\n        \"max\": 8.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          8.0,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fa2\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.5444267241439458,\n        \"min\": 0.0,\n        \"max\": 3.5937499999999996,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.78125,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4435316400826057,\n        \"min\": 1.25,\n        \"max\": 9.21875,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3.5937499999999996,\n          1.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lat_mean_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.100222015381966,\n        \"min\": 35.03471851348877,\n        \"max\": 92.11071014404297,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          35.03471851348877,\n          60.82291126251221\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lat_p50\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.153667322304182,\n        \"min\": 34.30628776550293,\n        \"max\": 68.52865219116211,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          34.30628776550293,\n          60.050010681152344\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lat_p95\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 74.97133224094743,\n        \"min\": 38.0706787109375,\n        \"max\": 220.32570838928223,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          38.0706787109375,\n          66.54858589172363\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lat_p99\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81.17742181221446,\n        \"min\": 44.88730430603027,\n        \"max\": 239.84813690185547,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          44.88730430603027,\n          70.20211219787598\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "registry = get_model_registry()\n",
        "results = []\n",
        "\n",
        "for name, cfg in registry.items():\n",
        "    print(\"=\"*80)\n",
        "    print(\"MODEL:\", name, \"|\", cfg[\"desc\"])\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model = cfg[\"builder\"]()\n",
        "    print(\"Params:\", count_parameters(model))\n",
        "\n",
        "    top1, top5 = evaluate_accuracy(model, val_loader, MAX_VAL_BATCHES)\n",
        "    print(f\"Accuracy ‚Äî Top-1: {top1:.2f}% | Top-5: {top5:.2f}%\")\n",
        "\n",
        "    lat = measure_latency(model)\n",
        "    print(\"Latency (ms):\", lat)\n",
        "\n",
        "    results.append({\n",
        "        \"model\": name,\n",
        "        \"desc\": cfg[\"desc\"],\n",
        "        \"bits\": cfg[\"bits\"],\n",
        "        \"fa2\": cfg[\"fa2\"],\n",
        "        \"top1\": top1,\n",
        "        \"top5\": top5,\n",
        "        \"lat_mean_ms\": lat[\"mean_ms\"],\n",
        "        \"lat_p50\": lat[\"p50_ms\"],\n",
        "        \"lat_p95\": lat[\"p95_ms\"],\n",
        "        \"lat_p99\": lat[\"p99_ms\"],\n",
        "    })\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ZHtKk0Q5sJ"
      },
      "source": [
        "# =====================================================================\n",
        "# üìå CELL 8 ‚Äî Save results CSV\n",
        "# =====================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xypUHTJIQ5sJ",
        "outputId": "d22e31df-ef8f-4594-9512-5c0e69399a6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/vit_quant_fa2_stage1_results.csv\n"
          ]
        }
      ],
      "source": [
        "csv_path = \"/content/vit_quant_fa2_stage1_results.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"Saved:\", csv_path)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
